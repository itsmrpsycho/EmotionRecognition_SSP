{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './EmoDB_dataset/wav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "def extract_mfcc_features(file_path: str, n_mfcc: int = 39, \n",
    "                          frame_size: float = 0.025, frame_stride: float = 0.01, \n",
    "                          n_segments: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracts 39 MFCC features framewise from an audio file and then applies\n",
    "    average pooling to condense the features over time into an n x 39 feature matrix.\n",
    "    \n",
    "    Parameters:\n",
    "      file_path (str): Path to the audio file.\n",
    "      n_mfcc (int): Number of MFCC features to extract. Default is 39.\n",
    "      frame_size (float): Length of each frame in seconds. Default is 0.025.\n",
    "      frame_stride (float): Step between successive frames in seconds. Default is 0.01.\n",
    "      n_segments (int): Number of segments (n) to pool the frames into.\n",
    "    \n",
    "    Returns:\n",
    "      np.ndarray: A n x 39 array where each row is the average MFCC vector for that segment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        signal, sample_rate = librosa.load(file_path, sr=None)\n",
    "        frame_length = int(frame_size * sample_rate)\n",
    "        hop_length = int(frame_stride * sample_rate)\n",
    "        \n",
    "        # Extract MFCC features; result shape is (n_mfcc, T) where T is number of frames.\n",
    "        mfcc = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=n_mfcc,\n",
    "                                    n_fft=frame_length, hop_length=hop_length)\n",
    "        \n",
    "        # Normalize the MFCC features along each coefficient dimension.\n",
    "        mfcc_normalized = mfcc - np.mean(mfcc, axis=1, keepdims=True)\n",
    "        \n",
    "        # Transpose to shape (T, n_mfcc) for pooling along the time axis.\n",
    "        mfcc_normalized = mfcc_normalized.T\n",
    "        \n",
    "        # Divide the frames into n_segments segments and compute the average for each segment.\n",
    "        segments = np.array_split(mfcc_normalized, n_segments, axis=0)\n",
    "        pooled_features = np.array([np.mean(seg, axis=0) for seg in segments])\n",
    "        \n",
    "        return pooled_features  # Shape: (n_segments, 39)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "    return np.array([])\n",
    "\n",
    "def process_directory_mfcc(directory: str, n_segments: int = 10) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Processes all .wav files in the given directory, extracting their MFCC features\n",
    "    using average pooling to produce an n x 39 feature matrix for each file.\n",
    "    \n",
    "    Parameters:\n",
    "      directory (str): Path to the directory containing .wav files.\n",
    "      n_segments (int): Number of segments to pool the frames into for each file.\n",
    "    \n",
    "    Returns:\n",
    "      Dict[str, np.ndarray]: A dictionary mapping filenames to their corresponding feature matrices.\n",
    "    \"\"\"\n",
    "    feature_vectors = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.wav'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            features = extract_mfcc_features(file_path, n_segments=n_segments)\n",
    "            if features.size > 0:\n",
    "                feature_vectors[filename] = features\n",
    "    return feature_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "535"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 85\n",
    "mfccFeatures = process_directory_mfcc(directory, n)\n",
    "\n",
    "len(mfccFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85, 39)\n",
      "(85, 39)\n"
     ]
    }
   ],
   "source": [
    "print(mfccFeatures[\"03a01Fa.wav\"].shape)\n",
    "print(mfccFeatures[\"03a02Fc.wav\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier on MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def load_labels(csv_file: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(csv_file)\n",
    "\n",
    "def prepare_dataset(features: dict, labels: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Constructs the dataset by matching each audio file's feature matrix with its label.\n",
    "    Since each file is represented as an n x 39 matrix (n segments by 39 features),\n",
    "    we flatten it into a 1D feature vector of length n*39.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for _, row in labels.iterrows():\n",
    "        file_id = row['Filename']\n",
    "        if file_id in features:\n",
    "            # Flatten the (n, 39) matrix to a 1D vector (n*39,)\n",
    "            feature_matrix = features[file_id]\n",
    "            feature_vector = feature_matrix.flatten()\n",
    "            X.append(feature_vector)\n",
    "            y.append(int(row['EmotionNumeric']))\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def train_and_evaluate(X, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train GMM Classifier\n",
    "    gmm_model = GaussianMixture(n_components=len(np.unique(y)), random_state=42)\n",
    "    gmm_model.fit(X_train)\n",
    "    gmm_predictions = gmm_model.predict(X_test)\n",
    "\n",
    "    # Train SVM Classifier\n",
    "    svm_model = SVC(kernel='linear', random_state=42)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "    # Evaluate classifiers\n",
    "    print(\"GMM Classifier Report:\")\n",
    "    print(classification_report(y_test, gmm_predictions))\n",
    "\n",
    "    print(\"SVM Classifier Report:\")\n",
    "    print(classification_report(y_test, svm_predictions))\n",
    "\n",
    "    # Optionally save models\n",
    "    # joblib.dump(gmm_model, 'gmm_model.pkl')\n",
    "    # joblib.dump(svm_model, 'svm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (535, 3315)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanni\\miniconda3\\envs\\coding2\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        18\n",
      "           1       0.00      0.00      0.00        20\n",
      "           2       0.11      0.25      0.15        12\n",
      "           3       0.15      0.21      0.18        14\n",
      "           4       0.06      0.06      0.06        18\n",
      "           5       0.09      0.22      0.12         9\n",
      "           6       0.15      0.12      0.14        16\n",
      "\n",
      "    accuracy                           0.10       107\n",
      "   macro avg       0.08      0.12      0.09       107\n",
      "weighted avg       0.07      0.10      0.08       107\n",
      "\n",
      "SVM Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.78      0.60        18\n",
      "           1       0.75      0.60      0.67        20\n",
      "           2       0.50      0.08      0.14        12\n",
      "           3       0.33      0.29      0.31        14\n",
      "           4       0.67      0.56      0.61        18\n",
      "           5       0.50      1.00      0.67         9\n",
      "           6       0.60      0.56      0.58        16\n",
      "\n",
      "    accuracy                           0.55       107\n",
      "   macro avg       0.55      0.55      0.51       107\n",
      "weighted avg       0.57      0.55      0.53       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels_csv_path = \"EmoDB_dataset/emotion_mapping_detailed.csv\"\n",
    "labels = load_labels(labels_csv_path)\n",
    "\n",
    "# Prepare the dataset: each feature matrix is flattened to become a vector\n",
    "X, y = prepare_dataset(mfccFeatures, labels)\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "\n",
    "train_and_evaluate(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCC and LP Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import lfilter\n",
    "from scipy.fftpack import dct\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fftpack import dct\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "def extract_rcc(frame: np.ndarray, order: int = 12, n_rcc: int = 12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract Residual Cepstral Coefficients (RCC) from a signal frame using LPC and residual signal.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: LPC Analysis - Calculate the LPC coefficients (Prediction Coefficients)\n",
    "        autocorr = np.correlate(frame, frame, mode='full')\n",
    "        autocorr = autocorr[len(autocorr)//2:]  # Keep second half (autocorrelation)\n",
    "        \n",
    "        if autocorr[0] == 0:\n",
    "            return np.zeros(n_rcc)  # Silent frame, return zero vector\n",
    "        \n",
    "        # Levinson-Durbin recursion to solve for LPC coefficients\n",
    "        a = np.zeros(order + 1)\n",
    "        e = autocorr[0]\n",
    "        k = np.zeros(order)\n",
    "\n",
    "        for i in range(order):\n",
    "            acc = autocorr[i + 1] - np.dot(a[1:i + 1], autocorr[i:0:-1])\n",
    "            ki = acc / e\n",
    "            k[i] = ki\n",
    "            a[1:i+1] -= ki * a[i:0:-1]\n",
    "            a[i + 1] = ki\n",
    "            e *= (1 - ki ** 2)\n",
    "\n",
    "        # Step 2: Compute the residual signal by filtering the frame using LPC coefficients\n",
    "        residual = lfilter(a, [1.0], frame)\n",
    "        \n",
    "        # Step 3: Apply Cepstral Analysis (DCT) to the residual signal\n",
    "        # We use the first n_rcc coefficients from the DCT of the log of the residual power spectrum\n",
    "        residual_power_spectrum = np.abs(np.fft.fft(residual)) ** 2\n",
    "        log_residual_spectrum = np.log(residual_power_spectrum + 1e-8)  # Log power spectrum\n",
    "\n",
    "        # Compute the DCT (Discrete Cosine Transform)\n",
    "        rcc = dct(log_residual_spectrum, type=2)[:n_rcc]\n",
    "        \n",
    "        return rcc\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting RCC: {e}\")\n",
    "        return np.zeros(n_rcc)  # Return zero vector in case of error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def compute_lp_residual_energy(frame: np.ndarray, order: int = 12) -> float:\n",
    "    \"\"\"\n",
    "    Compute the Linear Prediction (LP) residual energy of a signal frame using librosa.\n",
    "\n",
    "    Parameters:\n",
    "    frame (np.ndarray): The input frame of the signal.\n",
    "    order (int): The order of the LPC analysis (default is 12).\n",
    "\n",
    "    Returns:\n",
    "    float: The energy of the LP residual signal.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: LPC Analysis using librosa to compute LPC coefficients\n",
    "        a = librosa.lpc(frame, order=order)  # LPC coefficients (a[0] is the gain)\n",
    "\n",
    "        # Step 2: Compute the residual signal by filtering the frame using LPC coefficients\n",
    "        residual = lfilter(a, [1.0], frame)\n",
    "\n",
    "        # Step 3: Compute the energy of the residual signal (sum of squared values)\n",
    "        residual_energy = np.sum(residual ** 2)\n",
    "\n",
    "        return residual_energy\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error computing LP residual energy: {e}\")\n",
    "        return 0.0  # Return zero in case of error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.signal import lfilter\n",
    "from scipy.fftpack import dct\n",
    "\n",
    "def extract_rcc_lp_features(file_path: str,\n",
    "                            frame_size: float = 0.025,\n",
    "                            frame_stride: float = 0.01,\n",
    "                            target_rcc_segments: int = 100,\n",
    "                            target_lp_segments: int = 100,\n",
    "                            rcc_order: int = 12) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts RCC and LP residual energy framewise then condenses the features \n",
    "    into fixed-length feature matrices via average pooling.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the audio file.\n",
    "        frame_size (float): Frame duration in seconds.\n",
    "        frame_stride (float): Step between successive frames in seconds.\n",
    "        target_rcc_segments (int): Desired number of pooled segments for RCC features.\n",
    "        target_lp_segments (int): Desired number of pooled segments for LP residual features.\n",
    "        rcc_order (int): Number of RCC coefficients to extract per frame.\n",
    "    \n",
    "    Returns:\n",
    "        tuple:\n",
    "            np.ndarray: Pooled RCC features with shape (target_rcc_segments, rcc_order).\n",
    "            np.ndarray: Pooled LP residual energies with shape (target_lp_segments, 1).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        signal, sr = librosa.load(file_path, sr=None)\n",
    "        frame_length = int(frame_size * sr)\n",
    "        hop_length = int(frame_stride * sr)\n",
    "        \n",
    "        # Frame the signal: shape (number_of_frames, frame_length)\n",
    "        frames = librosa.util.frame(signal, frame_length=frame_length, hop_length=hop_length).T\n",
    "\n",
    "        rcc_list = []\n",
    "        lp_list = []\n",
    "        \n",
    "        for frame in frames:\n",
    "            # Apply windowing\n",
    "            frame = frame * np.hamming(len(frame))\n",
    "            \n",
    "            # RCC extraction: use LPC analysis then DCT of the log power spectrum\n",
    "            # (using the existing extract_rcc logic)\n",
    "            try:\n",
    "                # Compute autocorrelation for LPC\n",
    "                autocorr = np.correlate(frame, frame, mode='full')\n",
    "                autocorr = autocorr[len(autocorr)//2:]\n",
    "                if autocorr[0] == 0:\n",
    "                    rcc = np.zeros(rcc_order)\n",
    "                else:\n",
    "                    a = np.zeros(rcc_order + 1)\n",
    "                    e = autocorr[0]\n",
    "                    for i in range(rcc_order):\n",
    "                        acc = autocorr[i + 1] - np.dot(a[1:i+1], autocorr[i:0:-1])\n",
    "                        ki = acc / e\n",
    "                        a[1:i+1] -= ki * a[i:0:-1]\n",
    "                        a[i + 1] = ki\n",
    "                        e *= (1 - ki ** 2)\n",
    "                    # Residual signal\n",
    "                    residual = lfilter(a, [1.0], frame)\n",
    "                    # Compute RCC using DCT\n",
    "                    residual_power_spectrum = np.abs(np.fft.fft(residual)) ** 2\n",
    "                    log_residual_spectrum = np.log(residual_power_spectrum + 1e-8)\n",
    "                    rcc = dct(log_residual_spectrum, type=2)[:rcc_order]\n",
    "            except Exception as ex:\n",
    "                print(f\"Error in RCC extraction for frame: {ex}\")\n",
    "                rcc = np.zeros(rcc_order)\n",
    "            \n",
    "            # LP residual energy extraction: using librosa.lpc for LPC coefficients\n",
    "            try:\n",
    "                a_lp = librosa.lpc(frame, order=rcc_order) \n",
    "                residual_lp = lfilter(a_lp, [1.0], frame)\n",
    "                lp_energy = np.sum(residual_lp ** 2)\n",
    "            except Exception as ex:\n",
    "                print(f\"Error in LP energy computation for frame: {ex}\")\n",
    "                lp_energy = 0.0\n",
    "            \n",
    "            rcc_list.append(rcc)\n",
    "            lp_list.append([lp_energy])  # Keep as list for 2D array\n",
    "\n",
    "        rcc_array = np.array(rcc_list)  # shape: (num_frames, rcc_order)\n",
    "        lp_array = np.array(lp_list)      # shape: (num_frames, 1)\n",
    "        \n",
    "        # Average pool features to desired length for each feature type\n",
    "        # For RCC features\n",
    "        rcc_segments = np.array_split(rcc_array, target_rcc_segments, axis=0)\n",
    "        rcc_pooled = np.array([np.mean(seg, axis=0) for seg in rcc_segments])\n",
    "        \n",
    "        # For LP residual energy features\n",
    "        lp_segments = np.array_split(lp_array, target_lp_segments, axis=0)\n",
    "        lp_pooled = np.array([np.mean(seg, axis=0) for seg in lp_segments])\n",
    "        \n",
    "        return rcc_pooled, lp_pooled\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return np.zeros((target_rcc_segments, rcc_order)), np.zeros((target_lp_segments, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory_rcc_lp(directory: str,\n",
    "                             frame_size: float = 0.025,\n",
    "                             frame_stride: float = 0.01,\n",
    "                             target_rcc_segments: int = 100,\n",
    "                             target_lp_segments: int = 100,\n",
    "                             rcc_order: int = 12) -> dict:\n",
    "    \"\"\"\n",
    "    Processes all .wav files in the directory and extracts RCC and LP residual features.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Path to the directory containing .wav files.\n",
    "        frame_size (float): Frame duration in seconds.\n",
    "        frame_stride (float): Step between successive frames in seconds.\n",
    "        target_rcc_segments (int): Desired number of pooled segments for RCC features.\n",
    "        target_lp_segments (int): Desired number of pooled segments for LP residual features.\n",
    "        rcc_order (int): Number of RCC coefficients to extract per frame.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping of filename to {\"RCC\": <flattened RCC vector>,\n",
    "                                      \"LP\": <flattened LP vector>,\n",
    "                                      \"Full\": <concatenated RCC and LP features>}.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "\n",
    "    feature_vectors = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.wav'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            rcc, lp = extract_rcc_lp_features(\n",
    "                file_path,\n",
    "                frame_size=frame_size,\n",
    "                frame_stride=frame_stride,\n",
    "                target_rcc_segments=target_rcc_segments,\n",
    "                target_lp_segments=target_lp_segments,\n",
    "                rcc_order=rcc_order\n",
    "            )\n",
    "            rcc_flat = rcc.flatten()  # Shape: (target_rcc_segments * rcc_order,)\n",
    "            lp_flat = lp.flatten()    # Shape: (target_lp_segments,)\n",
    "\n",
    "            feature_vectors[filename] = {\n",
    "                \"RCC\": rcc_flat,\n",
    "                \"LP\": lp_flat,\n",
    "                \"Full\": np.concatenate([rcc_flat, lp_flat])\n",
    "            }\n",
    "\n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(features: dict, labels: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Prepares RCC, LP, and combined feature sets for classification.\n",
    "\n",
    "    Returns:\n",
    "        X_full (np.ndarray): RCC + LP.\n",
    "        X_rcc (np.ndarray): RCC only.\n",
    "        X_lp (np.ndarray): LP only.\n",
    "        y (np.ndarray): Emotion labels.\n",
    "    \"\"\"\n",
    "    X_full, X_rcc, X_lp, y = [], [], [], []\n",
    "\n",
    "    for _, row in labels.iterrows():\n",
    "        file_id = row['Filename']\n",
    "        if file_id in features:\n",
    "            feat = features[file_id]\n",
    "            X_full.append(feat[\"Full\"])\n",
    "            X_rcc.append(feat[\"RCC\"])\n",
    "            X_lp.append(feat[\"LP\"])\n",
    "            y.append(int(row['EmotionNumeric']))\n",
    "\n",
    "    return (\n",
    "        np.array(X_full),\n",
    "        np.array(X_rcc),\n",
    "        np.array(X_lp),\n",
    "        np.array(y)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_and_evaluate(X, y):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple classifiers: SVM, Random Forest, XGBoost, Logistic Regression, KNN.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    classifiers = {\n",
    "        \"SVM (Linear Kernel)\": SVC(kernel='linear', random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric='mlogloss', random_state=42),\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=20000, random_state=42),\n",
    "        \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5)\n",
    "    }\n",
    "\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\nTraining and evaluating: {name}\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full (RCC + LP) shape: (535, 240)\n",
      "RCC-only shape: (535, 120)\n",
      "LP-only shape: (535, 120)\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "directory = \"./EmoDB_dataset/wav\"  # Replace with your path\n",
    "labels_csv_path = \"./EmoDB_dataset/emotion_mapping_detailed.csv\"  # Replace with your CSV path\n",
    "\n",
    "# Load label CSV\n",
    "labels = pd.read_csv(labels_csv_path)\n",
    "\n",
    "# Extract RCC and LP features\n",
    "features = process_directory_rcc_lp(directory, target_rcc_segments = 10, target_lp_segments = 120)\n",
    "\n",
    "# Prepare feature sets\n",
    "X_full, X_rcc, X_lp, y = prepare_dataset(features, labels)\n",
    "\n",
    "# Check dataset shapes\n",
    "print(\"Full (RCC + LP) shape:\", X_full.shape)\n",
    "print(\"RCC-only shape:\", X_rcc.shape)\n",
    "print(\"LP-only shape:\", X_lp.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RCC + LP Residual ---\n",
      "\n",
      "Training and evaluating: SVM (Linear Kernel)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.67      0.65        18\n",
      "           1       0.67      0.50      0.57        20\n",
      "           2       0.43      0.25      0.32        12\n",
      "           3       0.47      0.64      0.55        14\n",
      "           4       0.59      0.56      0.57        18\n",
      "           5       0.56      0.56      0.56         9\n",
      "           6       0.57      0.75      0.65        16\n",
      "\n",
      "    accuracy                           0.57       107\n",
      "   macro avg       0.56      0.56      0.55       107\n",
      "weighted avg       0.57      0.57      0.56       107\n",
      "\n",
      "\n",
      "Training and evaluating: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.89      0.68        18\n",
      "           1       0.80      0.60      0.69        20\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.50      0.43      0.46        14\n",
      "           4       0.47      0.39      0.42        18\n",
      "           5       0.69      1.00      0.82         9\n",
      "           6       0.52      0.75      0.62        16\n",
      "\n",
      "    accuracy                           0.58       107\n",
      "   macro avg       0.50      0.58      0.53       107\n",
      "weighted avg       0.52      0.58      0.54       107\n",
      "\n",
      "\n",
      "Training and evaluating: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.89      0.73        18\n",
      "           1       0.67      0.40      0.50        20\n",
      "           2       0.43      0.25      0.32        12\n",
      "           3       0.58      0.50      0.54        14\n",
      "           4       0.53      0.50      0.51        18\n",
      "           5       0.69      1.00      0.82         9\n",
      "           6       0.55      0.69      0.61        16\n",
      "\n",
      "    accuracy                           0.59       107\n",
      "   macro avg       0.58      0.60      0.58       107\n",
      "weighted avg       0.58      0.59      0.57       107\n",
      "\n",
      "\n",
      "Training and evaluating: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60        18\n",
      "           1       0.56      0.45      0.50        20\n",
      "           2       0.18      0.17      0.17        12\n",
      "           3       0.32      0.43      0.36        14\n",
      "           4       0.50      0.44      0.47        18\n",
      "           5       0.64      0.78      0.70         9\n",
      "           6       0.50      0.69      0.58        16\n",
      "\n",
      "    accuracy                           0.49       107\n",
      "   macro avg       0.49      0.49      0.48       107\n",
      "weighted avg       0.51      0.49      0.49       107\n",
      "\n",
      "\n",
      "Training and evaluating: K-Nearest Neighbors\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.78      0.55        18\n",
      "           1       0.35      0.55      0.43        20\n",
      "           2       0.40      0.17      0.24        12\n",
      "           3       0.57      0.29      0.38        14\n",
      "           4       0.33      0.11      0.17        18\n",
      "           5       0.80      0.44      0.57         9\n",
      "           6       0.30      0.38      0.33        16\n",
      "\n",
      "    accuracy                           0.40       107\n",
      "   macro avg       0.45      0.39      0.38       107\n",
      "weighted avg       0.43      0.40      0.38       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- RCC + LP Residual ---\")\n",
    "train_and_evaluate(X_full, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RCC Only ---\n",
      "\n",
      "Training and evaluating: SVM (Linear Kernel)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.67      0.65        18\n",
      "           1       0.67      0.50      0.57        20\n",
      "           2       0.43      0.25      0.32        12\n",
      "           3       0.47      0.64      0.55        14\n",
      "           4       0.59      0.56      0.57        18\n",
      "           5       0.56      0.56      0.56         9\n",
      "           6       0.57      0.75      0.65        16\n",
      "\n",
      "    accuracy                           0.57       107\n",
      "   macro avg       0.56      0.56      0.55       107\n",
      "weighted avg       0.57      0.57      0.56       107\n",
      "\n",
      "\n",
      "Training and evaluating: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.89      0.64        18\n",
      "           1       0.59      0.65      0.62        20\n",
      "           2       0.50      0.08      0.14        12\n",
      "           3       0.45      0.36      0.40        14\n",
      "           4       0.75      0.50      0.60        18\n",
      "           5       0.75      1.00      0.86         9\n",
      "           6       0.38      0.38      0.38        16\n",
      "\n",
      "    accuracy                           0.55       107\n",
      "   macro avg       0.56      0.55      0.52       107\n",
      "weighted avg       0.56      0.55      0.52       107\n",
      "\n",
      "\n",
      "Training and evaluating: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.78      0.67        18\n",
      "           1       0.67      0.50      0.57        20\n",
      "           2       0.33      0.25      0.29        12\n",
      "           3       0.62      0.57      0.59        14\n",
      "           4       0.56      0.50      0.53        18\n",
      "           5       0.73      0.89      0.80         9\n",
      "           6       0.58      0.69      0.63        16\n",
      "\n",
      "    accuracy                           0.59       107\n",
      "   macro avg       0.58      0.60      0.58       107\n",
      "weighted avg       0.58      0.59      0.58       107\n",
      "\n",
      "\n",
      "Training and evaluating: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60        18\n",
      "           1       0.56      0.45      0.50        20\n",
      "           2       0.18      0.17      0.17        12\n",
      "           3       0.32      0.43      0.36        14\n",
      "           4       0.50      0.44      0.47        18\n",
      "           5       0.64      0.78      0.70         9\n",
      "           6       0.50      0.69      0.58        16\n",
      "\n",
      "    accuracy                           0.49       107\n",
      "   macro avg       0.49      0.49      0.48       107\n",
      "weighted avg       0.51      0.49      0.49       107\n",
      "\n",
      "\n",
      "Training and evaluating: K-Nearest Neighbors\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.78      0.55        18\n",
      "           1       0.35      0.55      0.43        20\n",
      "           2       0.40      0.17      0.24        12\n",
      "           3       0.57      0.29      0.38        14\n",
      "           4       0.33      0.11      0.17        18\n",
      "           5       0.80      0.44      0.57         9\n",
      "           6       0.30      0.38      0.33        16\n",
      "\n",
      "    accuracy                           0.40       107\n",
      "   macro avg       0.45      0.39      0.38       107\n",
      "weighted avg       0.43      0.40      0.38       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- RCC Only ---\")\n",
    "train_and_evaluate(X_rcc, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LP Residual Only ---\n",
      "\n",
      "Training and evaluating: SVM (Linear Kernel)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.61      0.39        18\n",
      "           1       0.50      0.15      0.23        20\n",
      "           2       0.14      0.08      0.11        12\n",
      "           3       0.20      0.07      0.11        14\n",
      "           4       0.75      0.17      0.27        18\n",
      "           5       0.29      0.22      0.25         9\n",
      "           6       0.30      0.75      0.43        16\n",
      "\n",
      "    accuracy                           0.31       107\n",
      "   macro avg       0.35      0.29      0.26       107\n",
      "weighted avg       0.38      0.31      0.27       107\n",
      "\n",
      "\n",
      "Training and evaluating: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.89      0.63        18\n",
      "           1       0.69      0.55      0.61        20\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.50      0.71      0.59        14\n",
      "           4       0.33      0.06      0.10        18\n",
      "           5       0.67      0.89      0.76         9\n",
      "           6       0.47      0.56      0.51        16\n",
      "\n",
      "    accuracy                           0.51       107\n",
      "   macro avg       0.45      0.52      0.46       107\n",
      "weighted avg       0.46      0.51      0.45       107\n",
      "\n",
      "\n",
      "Training and evaluating: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.61      0.55        18\n",
      "           1       0.71      0.50      0.59        20\n",
      "           2       0.00      0.00      0.00        12\n",
      "           3       0.38      0.43      0.40        14\n",
      "           4       0.40      0.22      0.29        18\n",
      "           5       0.69      1.00      0.82         9\n",
      "           6       0.42      0.69      0.52        16\n",
      "\n",
      "    accuracy                           0.48       107\n",
      "   macro avg       0.44      0.49      0.45       107\n",
      "weighted avg       0.46      0.48      0.45       107\n",
      "\n",
      "\n",
      "Training and evaluating: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.72      0.50        18\n",
      "           1       0.50      0.25      0.33        20\n",
      "           2       0.20      0.08      0.12        12\n",
      "           3       0.27      0.21      0.24        14\n",
      "           4       0.50      0.17      0.25        18\n",
      "           5       0.40      0.22      0.29         9\n",
      "           6       0.31      0.69      0.42        16\n",
      "\n",
      "    accuracy                           0.36       107\n",
      "   macro avg       0.37      0.34      0.31       107\n",
      "weighted avg       0.38      0.36      0.32       107\n",
      "\n",
      "\n",
      "Training and evaluating: K-Nearest Neighbors\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.61      0.45        18\n",
      "           1       0.38      0.45      0.41        20\n",
      "           2       0.33      0.08      0.13        12\n",
      "           3       0.50      0.21      0.30        14\n",
      "           4       0.33      0.06      0.10        18\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.22      0.50      0.30        16\n",
      "\n",
      "    accuracy                           0.31       107\n",
      "   macro avg       0.30      0.27      0.24       107\n",
      "weighted avg       0.32      0.31      0.27       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- LP Residual Only ---\")\n",
    "train_and_evaluate(X_lp, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
